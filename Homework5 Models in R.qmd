---
title: "Homework5 Models in R"
format: pdf
editor: visual
---

```{r, include=FALSE}
#Clearing the working environment
rm(list=ls())
```

## Task One: Conceptual Questions

### Question One: What is the purpose of using cross-validation when fitting a random forest model?

Cross-validation allows us to make use of the entire dataset that we have access to. It does this by creating equal sections of data (called "folds") and alternating which single fold is used for testing and what folds are grouped together for training. This reduces the chance of getting a weird model due to a weird training/testing split and gives a less variable estimate of the model since it uses more (all) of the data. 

### Question Two: Describe the bagged tree algorithm.

A bagged tree algorithm means that bootstrap aggregation is used in addition to the standard tree algorithm. Bootstrapping means that we are resampling either from the data (non-parametric) or from a fitted model (parametric). For each resample, we have a certain fitted tree. We can then use each tree to come up with a prediction, and then we can take the average of all of the predictions from all of our trees to create a final prediction. 

### Question Three: What is meant by a general linear model?

A general linear model does not necessarily assume that there is a linear relationship between the explanatory variable(s) and the response, but it assumes that there is a linear relationship between the explanatory variable(s) and the transformed response. This transformation is also referred to as the link function.

### Question Four: When fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?

An interaction term allows the model to account for the fact that the impact of an explanatory variable A on the response Y may be different at different levels of another explanatory variable B. Otherwise, the impacts of the two explanatory variables A and B will be independent and constant regardless of each other's values. 

### Question Five: Why do we split our data into a training and test set?

Splitting into a training and test set is important to maintain independence. If a model is trained and tested on the same data, it is obvious that the model will be a good fit to that data. We are actually curious about how the model performs on new data, so it is important to choose either to use the data for training or for testing but not both. 

\newpage
## Task Two: Data Prep

### Packages and data

```{r, warning=FALSE, message=FALSE}
#loading required packages
library("tidyverse")
library("tidymodels")
library("caret")
library("yardstick")

#reading in the heart data
heartdata <- read_csv("https://www4.stat.ncsu.edu/~online/datasets/heart.csv")
```

### Question One: Summarizing the heart data

```{r}
summary(heartdata) #creating a summary for all variables in the heart data
```

#### a. What type of variable (in R) is Heart Disease? Categorical or Quantitative?

Heart Disease is a numerical (integer) variable. It is quantitative rather than categorical, and R is assuming that all numeric values between 0 and 1 are valid. We know this because a numerical summary is presented for the Heart Disease variable. 

#### b. Does this make sense? Why or why not.

This does not make sense because a person has either been diagnosed with heart disease or they have not. Therefore, Heart Disease should be a categorical variable rather than a numeric. Heart Disease should be a factor variable with the levels 1 (indicating the person has been diagnosed with heart disease) and 0 (indicating the person has not been diagnosed with heart disease).

### Question Two: Managing and cleaning the heart data

```{r}
#Creating a new heart tibble
new_heart <- heartdata |> #using the initial tibble
  mutate(new_hd = as.factor(HeartDisease)) |> #creating a new factor heart disease variable
  select(-c(ST_Slope, HeartDisease)) #removing ST_Slope and old heart disease variable
```

\newpage
### Task Three: EDA

#### Question One: Creating a Scatter Plot Corresponding to the Heart Disease Model

```{r}
g <- ggplot(new_heart, aes(x = MaxHR, y = Age, color = new_hd)) + geom_point() + geom_smooth(method=lm, se=FALSE) + labs(title = "Age by Max Heart Rate", x = "Max Heart Rate", y = "Age")
g
```

#### Question Two: Based on visual evidence, do you think an interaction model or an additive model is
more appropriate? Justify your answer.

Based on this plot, an interaction model is more appropriate because the two lines are not parallel. That indicates that whether or not heart disease is present impacts the effect that maximum heart rate has on our response variable age, and therefore an interaction term is necessary. 

\newpage
### Task Four: Testing and Training

```{r}
set.seed(101) #setting a random seed so this process can be duplicated
heart_split <- initial_split(new_heart, prop = 0.80) #specifying a 80/20 split
heart_train <- training(heart_split) #splitting training data
heart_test <- testing(heart_split) #splitting testing data
```

\newpage
### Task Five: OLS and LASSO

#### Question One: Fitting an OLS Interation Model

```{r}
#fitting an interaction model using the training data
ols_mlr <- lm(Age ~ MaxHR + new_hd + MaxHR:new_hd, data = heart_train)

#displaying the model summary
summary(ols_mlr) 
```

#### Question Two: Calculating the RMSE for the OLS Interaction Model

```{r}
RMSE_calc <- sqrt(mean((heart_test$Age - predict(ols_mlr, newdata=heart_test))^2))
RMSE_calc
```

#### Question Three: Fitting a LASSO Model

```{r}
heart_cv_folds <- vfold_cv(heart_train, 10) #creating 10 fold CV of the training data

LASSO_recipe <- recipe(Age ~ MaxHR + new_hd, data = heart_train) |>
  step_normalize(MaxHR) |> #standardizing the quantitative variable
  step_dummy(new_hd) |> #standardizing the categorical variable
  step_interact(~MaxHR:starts_with("new_hd")) #creating the interaction 

LASSO_recipe
```

#### Question Four: Now, set up your appropriate spec, and grid. Next, select your final model, and report the results using the tidy() function around your model name.

```{r}
#Creating LASSO spec
LASSO_spec <- linear_reg(penalty = tune(), mixture = 1) |>
  set_engine("glmnet")

#Creating LASSO workflow
LASSO_wkf <- workflow () |>
  add_recipe(LASSO_recipe) |>
  add_model(LASSO_spec)

#Creating LASSO grid
LASSO_grid <- LASSO_wkf |>
  tune_grid(resamples = heart_cv_folds, grid = grid_regular(penalty(), levels = 200))

#Finding the lowest RMSE for all 200 LASSO models
lowest_rmse <- LASSO_grid |>
  select_best(metric = "rmse")

#Fitting the best model on the entire training dataset
LASSO_final <- LASSO_wkf |>
  finalize_workflow(lowest_rmse) |>
  fit(heart_train)

tidy(LASSO_final)
```

#### Question Five: Without looking at the RMSE calculations, would you expect the RMSE calculations to be roughly the same or different? Justify your answer using output from your LASSO model.

Without looking, I would expect the RMSE values very similar because the tuning parameter penalty is just 0.017 so it is fairly close to 0. 

#### Question Six: Now compare the RMSE between your OLS and LASSO model and show that the RMSE calculations were roughly the same.

The RMSE from the OLS Interaction Model was

```{r}
#Reprinting saved calculated RMSE value for the OLS model
RMSE_calc
```

The RMSE from the LASSO model is

```{r}
#Calculating the RMSE of the LASSO model
LASSO_final |>
  predict(heart_test) |>
  pull() |>
  rmse_vec(truth = heart_test$Age)
```

These values are extremely similar (off by just about 0.01)

#### Question Seven: Why are the RMSE calculations roughly the same if the coefficients for each model are different?

\newpage
### Task Six: Logistic Regression

#### Question One: Proposing two different logistic models

```{r}
#Creating the spec for logistic models
LR_spec <- logistic_reg() |>
  set_engine("glm")
```

```{r}
#Creating the first logistic model using Resting Blood Pressure, Sex, and Cholesterol as predictors
LR1_rec <- recipe(new_hd ~ RestingBP + Cholesterol + Sex, data = heart_train) |>
  step_normalize(all_numeric()) |>
  step_dummy(Sex)

LR1_wkf <- workflow() |>
  add_recipe(LR1_rec) |>
  add_model(LR_spec)

LR1_fit <- LR1_wkf |>
  fit_resamples(heart_cv_folds, metrics = metric_set(accuracy, mn_log_loss))
```

```{r}
#Creating the first logistic model using Max heart rate, Age, and Sex as predictors
LR2_rec <- recipe(new_hd ~ MaxHR + Age + Sex, data = heart_train) |>
  step_normalize(Age, MaxHR) |>
  step_dummy(Sex)

LR2_wkf <- workflow() |>
  add_recipe(LR2_rec) |>
  add_model(LR_spec)

LR2_fit <- LR2_wkf |>
  fit_resamples(heart_cv_folds, metrics = metric_set(accuracy, mn_log_loss))
```

```{r}
rbind(LR1_fit |> collect_metrics(),
      LR2_fit |> collect_metrics()) |>
  mutate(Model = c("Model1", "Model1", "Model2", "Model2")) |>
  select(Model, everything())
```

Based on the accuracy and log loss metrics above, Model 2 is a better model. This is because it has both a higher accuracy and a lower log loss. Model 2 predicts heart disease based off of Max heart rate, age, and sex. Model 2 only shared the sex predictor in common with Model 1 which used resting blood pressure and cholesterol as predictors. This is a significant finding because it indicates that Max HR and Age are potentially better predictors than Blood Pressure and Cholesterol. 

#### Question Two: Lastly, check how well your chosen model does on the test set using the confusionMatrix() function.

```{r}
LR_train_fit <- LR2_wkf |>
  fit(heart_train) 

conf_mat(heart_test |>
           mutate(estimate = LR_train_fit |>
                    predict(heart_test) |>
                    pull()),
         new_hd,
         estimate)
```

This confusion matrix shows that my model correctly predicted no heart disease diagnosis in 62 out of 94 cases. My model correctly predicted a heart disease diagnosis in 70 out of 90 heart disease cases. My model seems to do about as well at predicting heart disease cases vs predicting non-heart disease cases, but is maybe slightly better at predicting heart disease cases. 

#### Question Three: Next, identify the values of sensitivity and specificity, and interpret them in the context of the problem.

Sensitivity is probability that a test correctly detects a disease when it is truly present and is calculated by (# positive and test positive)/# total positive. In this context, sensitivity describes the probability of the model accurately detecting heart disease when a person is known to be diagnosed with heart disease. My model correctly predicted a heart disease diagnosis in 70 out of 90 heart disease cases. This is a sensitivity rate of 70/90 = 77.78%. Sensitivity is the probability that a test correctly does not detect disease when it is truly absent and is calculated by (# negative and test negative)/# total negative. In this context, specificity describes the probability of the model accurately not detecting heart disease when a person is known to not be diagnosed with heart disease. This confusion matrix shows that my model correctly predicted no heart disease diagnosis in 62 out of 94 cases. This is a specificity rate of 62/94 = 65.96%. 